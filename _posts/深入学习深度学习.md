# 前言

深入学习深度学习是李沐博士出版的书，最近偶然发现出了直播教程，正好最近的项目需要用到神经网络，所以跟着学习了一番。

# Week1

数据中的类别值或离散值，可以将`NaN`视为一个类别

```python
inputs = pandas.get_dummies(inputs,dummy_na = True)
```

Tensor的reshape是不会更换地址的

```python
a = torch.arange(12)
b = a.reshape((3,4))
b[:] = 2
a # 这里打印出来的a也是被改为元素均为2
```

神经网络的正向反向的复杂度其实是差不多的。

**如何存梯度以及读取梯度**

```python
x.requires_grad_(True)
x.grad.zero_() # 在下一次计算时要把梯度清零
y = 关于x的计算
y.backward()
x.grad # 访问梯度
```

# Week2

线性回归问题，线性模型可以看作是单层的神经网络

优化时，深度学习比较常用小批量随机梯度下降。因为在整个训练集上算梯度花费太高。因此可以随机采样一些样本计算近似损失。

线性回归notebook：https://zh-v2.d2l.ai/chapter_linear-networks/linear-regression-scratch.html

Softmax回归

回归和分类有较大的不同。分类问题通常有多个输出，输出i是预测第i类的置信度。

sofemax可以把正负值转化为概率，即预测为某一类的概率，相当于**多分类**。

# Week3

## 感知机

感觉是经典的神经网络结构？

output = wx+b

**多层感知机**：可以弥补单层感知机的缺陷，如感知机不能处理XOR分类问题。

## 为什么需要非线性激活函数？

因为没有激活函数，多层的感知机还是线性的变换，其实依然相当于一个单层的线性模型。

常用激活函数：Sigmoid ReLU Tanh

ReLU(x) = max(x,0)，为什么很多时候用relu，因为很简单，相比其他激活函数不用用到指数运算。

![](https://cdn.jsdelivr.net/gh/Jia-py/blog_picture/21_4/Snipaste_2021-04-18_13-34-18.jpg)

**一层隐藏层理论上可以拟合任何函数，为什么不是去增加隐藏层的神经元而是去增加层数呢？**因为，单层很多神经元的网络结构不容易训练。而多层较少神经元的网络结构比较容易训练。

## 模型选择

训练误差：模型在**训练数据**上的误差

泛化误差：模型在**新数据**上的误差**（我们关心的）**

验证数据集：一个用来评估模型好坏的数据集，可以调参。

测试数据集：只用一次的数据集，不能用来调参。

K-则交叉验证：在没有足够多的数据时使用（常态）。将数据分割为K块，使用第i块作为验证数据集，其余的作为训练数据集。

## 过拟合与欠拟合

在简单数据集上应用复杂（高容量）模型，容易过拟合。

在复杂数据集上应用简单（低容量）模型，容易欠拟合。

模型的容量指的是模型拟合函数的能力。

![](https://cdn.jsdelivr.net/gh/Jia-py/blog_picture/21_4/Snipaste_2021-04-18_14-53-44.jpg)

## QA

1. 超参数的设计？经验；比较推荐随机方法选超参数
2. 数据不均衡时如何选择训练数据集、验证集？验证集尽量保证是均衡分布的

# week4

## 权重衰退

通过限制w权重始终小于一个值(硬性限制，每一个w都小于某一个值)

![](https://cdn.jsdelivr.net/gh/Jia-py/blog_picture/21_4/Snipaste_2021-04-24_10-54-23.jpg)

与加入正则项的作用很相似(柔性的限制)

![](https://cdn.jsdelivr.net/gh/Jia-py/blog_picture/21_4/Snipaste_2021-04-24_10-54-55.jpg)

*为什么叫权重衰退*：每次在更新参数时，都先给（1-λn）w作一次缩小。λ是我们用来控制模型复杂度的超参数。

## QA

1. 为什么参数小就代表复杂度小呢？其实这里不是说参数小就复杂度小，只是在模型选取参数时，给它限制了参数选择的范围，使得模型的空间相对不作限制小了很多。
2. 一般权重衰退的值设置多少为好呢？一般取0.001

## Dropout

dropout有人说相当于一个正则。

对x加入噪音后，希望其期望不变，依然是x。

丢弃法对每个元素进行如下的扰动，将一些输出项随机置0来控制模型复杂度，常用于多层感知机的隐藏层输出上。

![](https://cdn.jsdelivr.net/gh/Jia-py/blog_picture/21_4/Snipaste_2021-04-24_11-50-17.jpg)

那么在这时，期望E = P*0 + (1-P)\*(x/(1-P)) = x

![](https://cdn.jsdelivr.net/gh/Jia-py/blog_picture/21_4/Snipaste_2021-04-24_11-53-26.jpg)

